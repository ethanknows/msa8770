{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2 TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import os\n",
    "import csv\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_dict = {}\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    newtokens = [w for w in tokens if w.isalnum()]\n",
    "    stems = stem_tokens(tokens, stemmer)\n",
    "    return stems\n",
    "\n",
    "filelist= (\"1.00\",\"2.00\",\"3.00\",\"4.00\",\"5.00\")\n",
    "for file in filelist:\n",
    "    text = \"\"\n",
    "    Homeworkcsvfile = open(\"Homework_1.csv\",\"r\")\n",
    "    csvreader = csv.reader(Homeworkcsvfile, delimiter = \"^\")\n",
    "    for row in csvreader:\n",
    "        if row[-1] == file:\n",
    "            text = text + row[-2]\n",
    "    lowers = text.lower()\n",
    "    no_punctuation = lowers.translate(None, string.punctuation)\n",
    "    token_dict[file] = no_punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1701)\t0.0449991131773\n",
      "  (0, 952)\t0.0253517154328\n",
      "  (0, 1885)\t0.0511676838119\n",
      "  (0, 820)\t0.0511676838119\n",
      "  (0, 1781)\t0.0598617958152\n",
      "  (0, 1591)\t0.0301364305393\n",
      "  (0, 1199)\t0.0761901216038\n",
      "  (0, 1268)\t0.0449991131773\n",
      "  (0, 758)\t0.0429241855074\n",
      "  (0, 712)\t0.0511676838119\n",
      "  (0, 175)\t0.0761901216038\n",
      "  (0, 1867)\t0.0728587527914\n",
      "  (0, 1641)\t0.0510254123998\n",
      "  (0, 1140)\t0.0253517154328\n",
      "  (0, 1605)\t0.0253517154328\n",
      "  (0, 243)\t0.0837261974702\n",
      "  (0, 1668)\t0.0936449483277\n",
      "  (0, 211)\t0.0510254123998\n",
      "  (0, 1334)\t0.036305001174\n",
      "  (0, 553)\t0.0253517154328\n",
      "  (0, 1813)\t0.0510254123998\n",
      "  (0, 1028)\t0.0301364305393\n",
      "  (0, 933)\t0.0253517154328\n",
      "  (0, 1565)\t0.0449991131773\n",
      "  (0, 1244)\t0.0795091935596\n",
      "  :\t:\n",
      "  (4, 934)\t0.0374614013138\n",
      "  (4, 1210)\t0.0374614013138\n",
      "  (4, 1507)\t0.0374614013138\n",
      "  (4, 1179)\t0.0374614013138\n",
      "  (4, 183)\t0.0374614013138\n",
      "  (4, 991)\t0.0374614013138\n",
      "  (4, 791)\t0.0374614013138\n",
      "  (4, 705)\t0.0374614013138\n",
      "  (4, 1489)\t0.0374614013138\n",
      "  (4, 729)\t0.0374614013138\n",
      "  (4, 75)\t0.0374614013138\n",
      "  (4, 80)\t0.0374614013138\n",
      "  (4, 386)\t0.0374614013138\n",
      "  (4, 1495)\t0.0374614013138\n",
      "  (4, 994)\t0.0634276660143\n",
      "  (4, 138)\t0.0374614013138\n",
      "  (4, 46)\t0.0374614013138\n",
      "  (4, 234)\t0.0374614013138\n",
      "  (4, 168)\t0.0374614013138\n",
      "  (4, 226)\t0.0374614013138\n",
      "  (4, 854)\t0.0374614013138\n",
      "  (4, 1632)\t0.0634276660143\n",
      "  (4, 602)\t0.0374614013138\n",
      "  (4, 835)\t0.0374614013138\n",
      "  (4, 1076)\t0.0374614013138\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(tokenizer=tokenize, stop_words='english',sublinear_tf=True)\n",
    "tfs = tfidf.fit_transform(token_dict.values())\n",
    "print tfs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine_similarities for 1 star document: [ 1.          0.32677021  0.25175293  0.34393201  0.35069962]\n",
      "cosine_similarities for 3 star document: [ 0.25175293  0.25474433  1.          0.25479472  0.27991655]\n",
      "cosine_similarities for 5 star document: [ 0.35069962  0.35137765  0.27991655  0.45622518  1.        ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "cosine_similarities_1_star = linear_kernel(tfs[0:1],tfs).flatten()\n",
    "cosine_similarities_3_star = linear_kernel(tfs[2:3],tfs).flatten()\n",
    "cosine_similarities_5_star = linear_kernel(tfs[4:5],tfs).flatten()\n",
    "print \"cosine_similarities for 1 star document:\" , cosine_similarities_1_star\n",
    "print \"cosine_similarities for 3 star document:\" , cosine_similarities_3_star\n",
    "print \"cosine_similarities for 5 star document:\" , cosine_similarities_5_star"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as we can see from the results:\n",
    "- For the 1 star comments, the most similar review documents is 5 star comments, with a cosine similarity 0.35069962.\n",
    "- For the 3 star comments, the most similar review documents is 5 star comments, with a cosine similarity 0.27991655.\n",
    "- For the 5 star comments, the most similar review documents is 4 star comments, with a cosine similarity 0.45622518."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
